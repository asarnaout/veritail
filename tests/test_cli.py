"""Tests for the CLI interface."""

from __future__ import annotations

from click.testing import CliRunner

from veritail.cli import main


class TestCLI:
    def test_init_creates_starter_files(self, tmp_path):
        runner = CliRunner()
        result = runner.invoke(main, ["init", "--dir", str(tmp_path)])

        assert result.exit_code == 0
        assert "Created" in result.output

        adapter_path = tmp_path / "adapter.py"
        queries_path = tmp_path / "queries.csv"

        assert adapter_path.exists()
        assert queries_path.exists()

        adapter_text = adapter_path.read_text(encoding="utf-8")
        assert "def search(query: str) -> list[SearchResult]:" in adapter_text
        assert "urlopen(request, timeout=10)" in adapter_text
        assert "SEARCH_API_URL" in adapter_text

        queries_text = queries_path.read_text(encoding="utf-8")
        assert "broad" in queries_text
        assert "navigational" in queries_text
        assert "long_tail" in queries_text
        assert "attribute" in queries_text

    def test_init_refuses_to_overwrite_without_force(self, tmp_path):
        adapter_path = tmp_path / "adapter.py"
        adapter_path.write_text("old adapter\n", encoding="utf-8")

        runner = CliRunner()
        result = runner.invoke(main, ["init", "--dir", str(tmp_path)])

        assert result.exit_code != 0
        assert "Refusing to overwrite existing file(s)" in result.output
        assert "--force" in result.output
        assert adapter_path.read_text(encoding="utf-8") == "old adapter\n"

    def test_init_force_overwrites_existing_files(self, tmp_path):
        adapter_path = tmp_path / "adapter.py"
        queries_path = tmp_path / "queries.csv"
        adapter_path.write_text("old adapter\n", encoding="utf-8")
        queries_path.write_text("old queries\n", encoding="utf-8")

        runner = CliRunner()
        result = runner.invoke(main, ["init", "--dir", str(tmp_path), "--force"])

        assert result.exit_code == 0
        content = adapter_path.read_text(encoding="utf-8")
        assert "Generated by `veritail init`" in content
        assert "query,type,category" in queries_path.read_text(encoding="utf-8")

    def test_init_supports_custom_file_names(self, tmp_path):
        runner = CliRunner()
        result = runner.invoke(
            main,
            [
                "init",
                "--dir",
                str(tmp_path),
                "--adapter-name",
                "my_adapter.py",
                "--queries-name",
                "my_queries.csv",
            ],
        )

        assert result.exit_code == 0
        assert (tmp_path / "my_adapter.py").exists()
        assert (tmp_path / "my_queries.csv").exists()

    def test_init_validates_file_extensions(self, tmp_path):
        runner = CliRunner()
        result = runner.invoke(
            main,
            ["init", "--dir", str(tmp_path), "--adapter-name", "adapter.txt"],
        )

        assert result.exit_code != 0
        assert "--adapter-name must end with .py" in result.output

    def test_help(self):
        runner = CliRunner()
        result = runner.invoke(main, ["--help"])
        assert result.exit_code == 0
        assert "veritail" in result.output

    def test_run_help(self):
        runner = CliRunner()
        result = runner.invoke(main, ["run", "--help"])
        assert result.exit_code == 0
        assert "--queries" in result.output
        assert "--adapter" in result.output
        assert "--config-name" in result.output
        assert "--vertical" in result.output

    def test_run_mismatched_adapters_and_configs(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")
        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text("def search(q): return []\n")

        runner = CliRunner()
        result = runner.invoke(
            main,
            [
                "run",
                "--queries",
                str(queries_file),
                "--adapter",
                str(adapter_file),
                "--config-name",
                "a",
                "--config-name",
                "b",
            ],
        )
        assert result.exit_code != 0
        assert "matching --config-name" in result.output

    def test_run_dual_config_with_only_one_name_fails(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_a = tmp_path / "adapter_a.py"
        adapter_a.write_text("def search(q): return []\n")
        adapter_b = tmp_path / "adapter_b.py"
        adapter_b.write_text("def search(q): return []\n")

        runner = CliRunner()
        result = runner.invoke(
            main,
            [
                "run",
                "--queries",
                str(queries_file),
                "--adapter",
                str(adapter_a),
                "--adapter",
                str(adapter_b),
                "--config-name",
                "only-one-name",
            ],
        )
        assert result.exit_code != 0
        assert "matching --config-name" in result.output

    def test_run_rejects_top_k_less_than_one(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text("def search(q): return []\n")

        runner = CliRunner()
        result = runner.invoke(
            main,
            [
                "run",
                "--queries",
                str(queries_file),
                "--adapter",
                str(adapter_file),
                "--top-k",
                "0",
            ],
        )
        assert result.exit_code != 0
        assert "--top-k must be >= 1." in result.output

    def test_run_single_config_with_file_backend(self, tmp_path, monkeypatch):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title='Shoe',\n"
            "        description='A shoe',\n"
            "        category='Shoes', price=50.0, position=0)]\n"
        )

        # Mock the LLM client to avoid needing real API keys
        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        mock_client = Mock(spec=LLMClient)
        mock_client.complete.return_value = LLMResponse(
            content="SCORE: 2\nREASONING: Good match",
            model="test-model",
            input_tokens=100,
            output_tokens=50,
        )

        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--config-name",
                    "test",
                    "--backend",
                    "file",
                    "--output-dir",
                    str(tmp_path / "results"),
                    "--llm-model",
                    "test-model",
                ],
            )

        assert result.exit_code == 0
        assert "ndcg" in result.output.lower() or "Evaluating" in result.output

    def test_run_single_config_auto_generates_config_name(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "my_adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title='Shoe',\n"
            "        description='A shoe',\n"
            "        category='Shoes', price=50.0, position=0)]\n"
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        mock_client = Mock(spec=LLMClient)
        mock_client.complete.return_value = LLMResponse(
            content="SCORE: 2\nREASONING: Good match",
            model="test-model",
            input_tokens=100,
            output_tokens=50,
        )

        output_dir = tmp_path / "results"
        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--backend",
                    "file",
                    "--output-dir",
                    str(output_dir),
                    "--llm-model",
                    "test-model",
                ],
            )

        assert result.exit_code == 0
        assert "No --config-name provided. Using generated names:" in result.output

        experiment_dirs = [p for p in output_dir.iterdir() if p.is_dir()]
        assert len(experiment_dirs) == 1
        assert experiment_dirs[0].name.startswith("my-adapter-")
        assert (experiment_dirs[0] / "metrics.json").exists()
        assert (experiment_dirs[0] / "report.html").exists()

    def test_run_help_shows_checks_option(self):
        runner = CliRunner()
        result = runner.invoke(main, ["run", "--help"])
        assert result.exit_code == 0
        assert "--checks" in result.output

    def test_run_with_custom_checks(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title='Shoe',\n"
            "        description='A shoe',\n"
            "        category='Shoes', price=50.0, position=0)]\n"
        )

        check_file = tmp_path / "my_checks.py"
        check_file.write_text(
            "from veritail.types import CheckResult, QueryEntry, SearchResult\n"
            "\n"
            "def check_custom(\n"
            "    query: QueryEntry, results: list[SearchResult]\n"
            ") -> list[CheckResult]:\n"
            "    return [CheckResult(\n"
            "        check_name='custom',\n"
            "        query=query.query,\n"
            "        product_id=None,\n"
            "        passed=True,\n"
            "        detail='ok',\n"
            "    )]\n"
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        mock_client = Mock(spec=LLMClient)
        mock_client.complete.return_value = LLMResponse(
            content="SCORE: 2\nREASONING: Good match",
            model="test-model",
            input_tokens=100,
            output_tokens=50,
        )

        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--config-name",
                    "test",
                    "--backend",
                    "file",
                    "--output-dir",
                    str(tmp_path / "results"),
                    "--llm-model",
                    "test-model",
                    "--checks",
                    str(check_file),
                ],
            )

        assert result.exit_code == 0
        assert "Loaded 1 custom check(s)" in result.output

    def test_run_help_shows_sample_option(self):
        runner = CliRunner()
        result = runner.invoke(main, ["run", "--help"])
        assert result.exit_code == 0
        assert "--sample" in result.output

    def test_run_rejects_sample_less_than_one(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text("def search(q): return []\n")

        runner = CliRunner()
        result = runner.invoke(
            main,
            [
                "run",
                "--queries",
                str(queries_file),
                "--adapter",
                str(adapter_file),
                "--sample",
                "0",
            ],
        )
        assert result.exit_code != 0
        assert "--sample must be >= 1" in result.output

    def test_run_sample_selects_subset(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\nboots\nsandals\nsneakers\nloafers\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title=q,\n"
            "        description='A product',\n"
            "        category='Footwear', price=50.0, position=0)]\n"
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        mock_client = Mock(spec=LLMClient)
        mock_client.complete.return_value = LLMResponse(
            content="SCORE: 2\nREASONING: Good match",
            model="test-model",
            input_tokens=100,
            output_tokens=50,
        )

        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--config-name",
                    "test",
                    "--backend",
                    "file",
                    "--output-dir",
                    str(tmp_path / "results"),
                    "--llm-model",
                    "test-model",
                    "--sample",
                    "2",
                ],
            )

        assert result.exit_code == 0
        assert "Sampled 2 of 5 queries" in result.output
        # Only 2 queries evaluated â†’ only 2 LLM calls
        assert mock_client.complete.call_count == 2

    def test_run_sample_gte_total_uses_all(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\nboots\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title=q,\n"
            "        description='A product',\n"
            "        category='Footwear', price=50.0, position=0)]\n"
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        mock_client = Mock(spec=LLMClient)
        mock_client.complete.return_value = LLMResponse(
            content="SCORE: 2\nREASONING: Good match",
            model="test-model",
            input_tokens=100,
            output_tokens=50,
        )

        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--config-name",
                    "test",
                    "--backend",
                    "file",
                    "--output-dir",
                    str(tmp_path / "results"),
                    "--llm-model",
                    "test-model",
                    "--sample",
                    "10",
                ],
            )

        assert result.exit_code == 0
        # --sample 10 >= 2 total queries, so all are used
        assert "Loaded 2 queries" in result.output
        assert mock_client.complete.call_count == 2

    def test_run_sample_is_deterministic(self, tmp_path):
        """Same --sample N with same query file should pick the same queries."""
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nalpha\nbravo\ncharlie\ndelta\necho\nfoxtrot\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title=q,\n"
            "        description='A product',\n"
            "        category='Test', price=10.0, position=0)]\n"
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        queries_seen: list[list[str]] = []

        def capture_calls():
            mock_client = Mock(spec=LLMClient)
            mock_client.complete.return_value = LLMResponse(
                content="SCORE: 2\nREASONING: Good match",
                model="test-model",
                input_tokens=100,
                output_tokens=50,
            )
            return mock_client

        for run_idx in range(2):
            mock_client = capture_calls()
            with patch("veritail.cli.create_llm_client", return_value=mock_client):
                runner = CliRunner()
                result = runner.invoke(
                    main,
                    [
                        "run",
                        "--queries",
                        str(queries_file),
                        "--adapter",
                        str(adapter_file),
                        "--config-name",
                        f"test-{run_idx}",
                        "--backend",
                        "file",
                        "--output-dir",
                        str(tmp_path / f"results-{run_idx}"),
                        "--llm-model",
                        "test-model",
                        "--sample",
                        "3",
                    ],
                )
            assert result.exit_code == 0
            # Extract which queries were judged from the LLM call args
            called_queries = [
                call.args[1]  # user_prompt
                for call in mock_client.complete.call_args_list
            ]
            queries_seen.append(called_queries)

        # Both runs should have called the LLM with the exact same prompts
        assert queries_seen[0] == queries_seen[1]

    def test_run_aborts_on_preflight_check_failure(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text("def search(q): return []\n")

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient

        mock_client = Mock(spec=LLMClient)
        mock_client.preflight_check.side_effect = RuntimeError(
            "Anthropic API key is invalid. "
            "Check your ANTHROPIC_API_KEY environment variable."
        )

        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--config-name",
                    "test",
                    "--output-dir",
                    str(tmp_path / "results"),
                ],
            )

        assert result.exit_code != 0
        assert "API key is invalid" in result.output

    def test_version(self):
        runner = CliRunner()
        result = runner.invoke(main, ["--version"])
        assert result.exit_code == 0
        assert "0.1.0" in result.output
