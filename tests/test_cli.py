"""Tests for the CLI interface."""

from __future__ import annotations

from click.testing import CliRunner

from veritail.cli import main
from veritail.types import MetricResult


class TestCLI:
    def test_init_creates_starter_files(self, tmp_path):
        runner = CliRunner()
        result = runner.invoke(main, ["init", "--dir", str(tmp_path)])

        assert result.exit_code == 0
        assert "Created" in result.output

        adapter_path = tmp_path / "adapter.py"
        queries_path = tmp_path / "queries.csv"

        assert adapter_path.exists()
        assert queries_path.exists()

        adapter_text = adapter_path.read_text(encoding="utf-8")
        assert "def search(query: str) -> SearchResponse:" in adapter_text
        assert "urlopen(request, timeout=10)" in adapter_text
        assert "SEARCH_API_URL" in adapter_text

        queries_text = queries_path.read_text(encoding="utf-8")
        assert "broad" in queries_text
        assert "navigational" in queries_text
        assert "long_tail" in queries_text
        assert "attribute" in queries_text

    def test_init_refuses_to_overwrite_without_force(self, tmp_path):
        adapter_path = tmp_path / "adapter.py"
        adapter_path.write_text("old adapter\n", encoding="utf-8")

        runner = CliRunner()
        result = runner.invoke(main, ["init", "--dir", str(tmp_path)])

        assert result.exit_code != 0
        assert "Refusing to overwrite existing file(s)" in result.output
        assert "--force" in result.output
        assert adapter_path.read_text(encoding="utf-8") == "old adapter\n"

    def test_init_force_overwrites_existing_files(self, tmp_path):
        adapter_path = tmp_path / "adapter.py"
        queries_path = tmp_path / "queries.csv"
        adapter_path.write_text("old adapter\n", encoding="utf-8")
        queries_path.write_text("old queries\n", encoding="utf-8")

        runner = CliRunner()
        result = runner.invoke(main, ["init", "--dir", str(tmp_path), "--force"])

        assert result.exit_code == 0
        content = adapter_path.read_text(encoding="utf-8")
        assert "Generated by `veritail init`" in content
        assert "query,type,category" in queries_path.read_text(encoding="utf-8")

    def test_init_supports_custom_file_names(self, tmp_path):
        runner = CliRunner()
        result = runner.invoke(
            main,
            [
                "init",
                "--dir",
                str(tmp_path),
                "--adapter-name",
                "my_adapter.py",
                "--queries-name",
                "my_queries.csv",
            ],
        )

        assert result.exit_code == 0
        assert (tmp_path / "my_adapter.py").exists()
        assert (tmp_path / "my_queries.csv").exists()

    def test_init_validates_file_extensions(self, tmp_path):
        runner = CliRunner()
        result = runner.invoke(
            main,
            ["init", "--dir", str(tmp_path), "--adapter-name", "adapter.txt"],
        )

        assert result.exit_code != 0
        assert "--adapter-name must end with .py" in result.output

    def test_help(self):
        runner = CliRunner()
        result = runner.invoke(main, ["--help"])
        assert result.exit_code == 0
        assert "veritail" in result.output

    def test_vertical_list(self):
        runner = CliRunner()
        result = runner.invoke(main, ["vertical", "list"])
        assert result.exit_code == 0
        assert "home-improvement" in result.output
        assert "fashion" in result.output
        assert "electronics" in result.output
        # Verify sorted order
        lines = result.output.strip().splitlines()
        assert lines == sorted(lines)

    def test_vertical_show(self):
        runner = CliRunner()
        result = runner.invoke(main, ["vertical", "show", "fashion"])
        assert result.exit_code == 0
        # Should contain actual vertical content
        assert len(result.output) > 100

    def test_vertical_show_unknown(self):
        runner = CliRunner()
        result = runner.invoke(main, ["vertical", "show", "nonexistent"])
        assert result.exit_code != 0
        assert "Unknown vertical" in result.output

    def test_vertical_show_pipe_to_file(self, tmp_path):
        """Verify output is plain text suitable for shell redirection."""
        runner = CliRunner()
        result = runner.invoke(main, ["vertical", "show", "home-improvement"])
        assert result.exit_code == 0
        # Write to file to simulate: veritail vertical show home-improvement > file.txt
        out_file = tmp_path / "my_vertical.txt"
        out_file.write_text(result.output, encoding="utf-8")
        # The file should be usable as a custom vertical
        assert out_file.read_text(encoding="utf-8") == result.output

    def test_run_context_from_file(self, tmp_path):
        """--context accepts a file path and reads its contents."""
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title='Shoe',\n"
            "        description='A shoe',\n"
            "        category='Shoes', price=50.0, position=0)]\n"
        )

        context_file = tmp_path / "context.txt"
        context_file.write_text(
            "We are a B2B commercial HVAC distributor.\n"
            "HVAC queries require strict part number matching."
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        mock_client = Mock(spec=LLMClient)
        mock_client.complete.return_value = LLMResponse(
            content="SCORE: 2\nREASONING: Good match",
            model="test-model",
            input_tokens=100,
            output_tokens=50,
        )

        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--config-name",
                    "test",
                    "--backend",
                    "file",
                    "--output-dir",
                    str(tmp_path / "results"),
                    "--llm-model",
                    "test-model",
                    "--context",
                    str(context_file),
                ],
            )

        assert result.exit_code == 0
        # Verify the file contents were passed to the LLM, not the file path
        call_args = mock_client.complete.call_args
        system_prompt = call_args.args[0]
        assert "HVAC" in system_prompt
        assert str(context_file) not in system_prompt

    def test_run_help(self):
        runner = CliRunner()
        result = runner.invoke(main, ["run", "--help"])
        assert result.exit_code == 0
        assert "--queries" in result.output
        assert "--adapter" in result.output
        assert "--config-name" in result.output
        assert "--vertical" in result.output

    def test_run_mismatched_adapters_and_configs(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")
        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text("def search(q): return []\n")

        runner = CliRunner()
        result = runner.invoke(
            main,
            [
                "run",
                "--queries",
                str(queries_file),
                "--adapter",
                str(adapter_file),
                "--llm-model",
                "test-model",
                "--config-name",
                "a",
                "--config-name",
                "b",
            ],
        )
        assert result.exit_code != 0
        assert "matching --config-name" in result.output

    def test_run_dual_config_with_only_one_name_fails(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_a = tmp_path / "adapter_a.py"
        adapter_a.write_text("def search(q): return []\n")
        adapter_b = tmp_path / "adapter_b.py"
        adapter_b.write_text("def search(q): return []\n")

        runner = CliRunner()
        result = runner.invoke(
            main,
            [
                "run",
                "--queries",
                str(queries_file),
                "--adapter",
                str(adapter_a),
                "--adapter",
                str(adapter_b),
                "--llm-model",
                "test-model",
                "--config-name",
                "only-one-name",
            ],
        )
        assert result.exit_code != 0
        assert "matching --config-name" in result.output

    def test_run_rejects_top_k_less_than_one(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text("def search(q): return []\n")

        runner = CliRunner()
        result = runner.invoke(
            main,
            [
                "run",
                "--queries",
                str(queries_file),
                "--adapter",
                str(adapter_file),
                "--llm-model",
                "test-model",
                "--top-k",
                "0",
            ],
        )
        assert result.exit_code != 0
        assert "--top-k must be >= 1." in result.output

    def test_run_single_config_with_file_backend(self, tmp_path, monkeypatch):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title='Shoe',\n"
            "        description='A shoe',\n"
            "        category='Shoes', price=50.0, position=0)]\n"
        )

        # Mock the LLM client to avoid needing real API keys
        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        mock_client = Mock(spec=LLMClient)
        mock_client.complete.return_value = LLMResponse(
            content="SCORE: 2\nREASONING: Good match",
            model="test-model",
            input_tokens=100,
            output_tokens=50,
        )

        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--config-name",
                    "test",
                    "--backend",
                    "file",
                    "--output-dir",
                    str(tmp_path / "results"),
                    "--llm-model",
                    "test-model",
                ],
            )

        assert result.exit_code == 0
        assert "ndcg" in result.output.lower() or "Evaluating" in result.output

    def test_run_single_config_auto_generates_config_name(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "my_adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title='Shoe',\n"
            "        description='A shoe',\n"
            "        category='Shoes', price=50.0, position=0)]\n"
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        mock_client = Mock(spec=LLMClient)
        mock_client.complete.return_value = LLMResponse(
            content="SCORE: 2\nREASONING: Good match",
            model="test-model",
            input_tokens=100,
            output_tokens=50,
        )

        output_dir = tmp_path / "results"
        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--backend",
                    "file",
                    "--output-dir",
                    str(output_dir),
                    "--llm-model",
                    "test-model",
                ],
            )

        assert result.exit_code == 0
        assert "No --config-name provided. Using generated names:" in result.output

        experiment_dirs = [p for p in output_dir.iterdir() if p.is_dir()]
        assert len(experiment_dirs) == 1
        assert experiment_dirs[0].name.startswith("my-adapter-")
        assert (experiment_dirs[0] / "metrics.json").exists()
        assert (experiment_dirs[0] / "report.html").exists()

    def test_run_help_shows_llm_base_url_option(self):
        runner = CliRunner()
        result = runner.invoke(main, ["run", "--help"])
        assert result.exit_code == 0
        assert "--llm-base-url" in result.output
        assert "--llm-api-key" in result.output

    def test_generate_queries_help_shows_llm_base_url_option(self):
        runner = CliRunner()
        result = runner.invoke(main, ["generate-queries", "--help"])
        assert result.exit_code == 0
        assert "--llm-base-url" in result.output
        assert "--llm-api-key" in result.output

    def test_run_with_llm_base_url(self, tmp_path):
        """--llm-base-url and --llm-api-key are forwarded to create_llm_client."""
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title='Shoe',\n"
            "        description='A shoe',\n"
            "        category='Shoes', price=50.0, position=0)]\n"
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        mock_client = Mock(spec=LLMClient)
        mock_client.complete.return_value = LLMResponse(
            content="SCORE: 2\nREASONING: Good match",
            model="qwen3:14b",
            input_tokens=100,
            output_tokens=50,
        )

        with patch(
            "veritail.cli.create_llm_client", return_value=mock_client
        ) as mock_create:
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--config-name",
                    "test",
                    "--backend",
                    "file",
                    "--output-dir",
                    str(tmp_path / "results"),
                    "--llm-model",
                    "qwen3:14b",
                    "--llm-base-url",
                    "http://localhost:11434/v1",
                    "--llm-api-key",
                    "not-needed",
                ],
            )

        assert result.exit_code == 0
        mock_create.assert_called_once_with(
            "qwen3:14b",
            base_url="http://localhost:11434/v1",
            api_key="not-needed",
        )

    def test_run_warns_on_unrecognized_model_with_base_url(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title='Shoe',\n"
            "        description='A shoe',\n"
            "        category='Shoes', price=50.0, position=0)]\n"
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        mock_client = Mock(spec=LLMClient)
        mock_client.complete.return_value = LLMResponse(
            content="SCORE: 2\nREASONING: Good match",
            model="llama3.1:8b",
            input_tokens=100,
            output_tokens=50,
        )

        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--config-name",
                    "test",
                    "--backend",
                    "file",
                    "--output-dir",
                    str(tmp_path / "results"),
                    "--llm-model",
                    "llama3.1:8b",
                    "--llm-base-url",
                    "http://localhost:11434/v1",
                    "--llm-api-key",
                    "ollama",
                ],
            )

        assert result.exit_code == 0
        assert "custom endpoint" in result.output
        assert "70B+" in result.output

    def test_run_help_shows_checks_option(self):
        runner = CliRunner()
        result = runner.invoke(main, ["run", "--help"])
        assert result.exit_code == 0
        assert "--checks" in result.output

    def test_run_with_custom_checks(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title='Shoe',\n"
            "        description='A shoe',\n"
            "        category='Shoes', price=50.0, position=0)]\n"
        )

        check_file = tmp_path / "my_checks.py"
        check_file.write_text(
            "from veritail.types import CheckResult, QueryEntry, SearchResult\n"
            "\n"
            "def check_custom(\n"
            "    query: QueryEntry, results: list[SearchResult]\n"
            ") -> list[CheckResult]:\n"
            "    return [CheckResult(\n"
            "        check_name='custom',\n"
            "        query=query.query,\n"
            "        product_id=None,\n"
            "        passed=True,\n"
            "        detail='ok',\n"
            "    )]\n"
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        mock_client = Mock(spec=LLMClient)
        mock_client.complete.return_value = LLMResponse(
            content="SCORE: 2\nREASONING: Good match",
            model="test-model",
            input_tokens=100,
            output_tokens=50,
        )

        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--config-name",
                    "test",
                    "--backend",
                    "file",
                    "--output-dir",
                    str(tmp_path / "results"),
                    "--llm-model",
                    "test-model",
                    "--checks",
                    str(check_file),
                ],
            )

        assert result.exit_code == 0
        assert "Loaded 1 custom check(s)" in result.output

    def test_run_help_shows_sample_option(self):
        runner = CliRunner()
        result = runner.invoke(main, ["run", "--help"])
        assert result.exit_code == 0
        assert "--sample" in result.output

    def test_run_rejects_sample_less_than_one(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text("def search(q): return []\n")

        runner = CliRunner()
        result = runner.invoke(
            main,
            [
                "run",
                "--queries",
                str(queries_file),
                "--adapter",
                str(adapter_file),
                "--llm-model",
                "test-model",
                "--sample",
                "0",
            ],
        )
        assert result.exit_code != 0
        assert "--sample must be >= 1" in result.output

    def test_run_sample_selects_subset(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\nboots\nsandals\nsneakers\nloafers\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title=q,\n"
            "        description='A product',\n"
            "        category='Footwear', price=50.0, position=0)]\n"
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        mock_client = Mock(spec=LLMClient)
        mock_client.complete.return_value = LLMResponse(
            content="SCORE: 2\nREASONING: Good match",
            model="test-model",
            input_tokens=100,
            output_tokens=50,
        )

        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--config-name",
                    "test",
                    "--backend",
                    "file",
                    "--output-dir",
                    str(tmp_path / "results"),
                    "--llm-model",
                    "test-model",
                    "--sample",
                    "2",
                ],
            )

        assert result.exit_code == 0
        assert "Sampled 2 of 5 queries" in result.output
        # Only 2 queries evaluated â†’ only 2 LLM calls
        assert mock_client.complete.call_count == 2

    def test_run_sample_gte_total_uses_all(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\nboots\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title=q,\n"
            "        description='A product',\n"
            "        category='Footwear', price=50.0, position=0)]\n"
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        mock_client = Mock(spec=LLMClient)
        mock_client.complete.return_value = LLMResponse(
            content="SCORE: 2\nREASONING: Good match",
            model="test-model",
            input_tokens=100,
            output_tokens=50,
        )

        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--config-name",
                    "test",
                    "--backend",
                    "file",
                    "--output-dir",
                    str(tmp_path / "results"),
                    "--llm-model",
                    "test-model",
                    "--sample",
                    "10",
                ],
            )

        assert result.exit_code == 0
        # --sample 10 >= 2 total queries, so all are used
        assert "Loaded 2 queries" in result.output
        assert mock_client.complete.call_count == 2

    def test_run_sample_is_deterministic(self, tmp_path):
        """Same --sample N with same query file should pick the same queries."""
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nalpha\nbravo\ncharlie\ndelta\necho\nfoxtrot\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title=q,\n"
            "        description='A product',\n"
            "        category='Test', price=10.0, position=0)]\n"
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        queries_seen: list[list[str]] = []

        def capture_calls():
            mock_client = Mock(spec=LLMClient)
            mock_client.complete.return_value = LLMResponse(
                content="SCORE: 2\nREASONING: Good match",
                model="test-model",
                input_tokens=100,
                output_tokens=50,
            )
            return mock_client

        for run_idx in range(2):
            mock_client = capture_calls()
            with patch("veritail.cli.create_llm_client", return_value=mock_client):
                runner = CliRunner()
                result = runner.invoke(
                    main,
                    [
                        "run",
                        "--queries",
                        str(queries_file),
                        "--adapter",
                        str(adapter_file),
                        "--config-name",
                        f"test-{run_idx}",
                        "--backend",
                        "file",
                        "--output-dir",
                        str(tmp_path / f"results-{run_idx}"),
                        "--llm-model",
                        "test-model",
                        "--sample",
                        "3",
                    ],
                )
            assert result.exit_code == 0
            # Extract which queries were judged from the LLM call args
            called_queries = [
                call.args[1]  # user_prompt
                for call in mock_client.complete.call_args_list
            ]
            queries_seen.append(called_queries)

        # Both runs should have called the LLM with the exact same prompts
        assert queries_seen[0] == queries_seen[1]

    def test_run_aborts_on_preflight_check_failure(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text("def search(q): return []\n")

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient

        mock_client = Mock(spec=LLMClient)
        mock_client.preflight_check.side_effect = RuntimeError(
            "Anthropic API key is invalid. "
            "Check your ANTHROPIC_API_KEY environment variable."
        )

        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--llm-model",
                    "test-model",
                    "--config-name",
                    "test",
                    "--output-dir",
                    str(tmp_path / "results"),
                ],
            )

        assert result.exit_code != 0
        assert "API key is invalid" in result.output

    def test_run_requires_llm_model_for_search(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text("def search(q): return []\n")

        runner = CliRunner()
        result = runner.invoke(
            main,
            [
                "run",
                "--queries",
                str(queries_file),
                "--adapter",
                str(adapter_file),
            ],
        )
        assert result.exit_code != 0
        assert "--llm-model is required when --queries is provided" in result.output

    def test_version(self):
        runner = CliRunner()
        result = runner.invoke(main, ["--version"])
        assert result.exit_code == 0
        assert "0.1.0" in result.output

    def test_run_batch_flag_in_help(self):
        runner = CliRunner()
        result = runner.invoke(main, ["run", "--help"])
        assert result.exit_code == 0
        assert "--batch" in result.output

    def test_run_batch_rejects_base_url(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text("def search(q): return []\n")

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient

        mock_client = Mock(spec=LLMClient)
        mock_client.supports_batch.return_value = True

        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--llm-model",
                    "gpt-4o",
                    "--llm-base-url",
                    "http://localhost:11434/v1",
                    "--batch",
                ],
            )

        assert result.exit_code != 0
        assert "--batch cannot be used with --llm-base-url" in result.output

    def test_run_batch_invokes_batch_pipeline(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title='Shoe',\n"
            "        description='A shoe',\n"
            "        category='Shoes', price=50.0, position=0)]\n"
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient

        mock_client = Mock(spec=LLMClient)
        mock_client.supports_batch.return_value = True

        mock_metrics = [
            MetricResult(metric_name="ndcg@10", value=0.8),
            MetricResult(metric_name="mrr", value=0.7),
        ]

        with (
            patch("veritail.cli.create_llm_client", return_value=mock_client),
            patch(
                "veritail.cli.run_batch_evaluation",
                return_value=([], [], mock_metrics, []),
            ) as mock_batch,
        ):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--config-name",
                    "test",
                    "--backend",
                    "file",
                    "--output-dir",
                    str(tmp_path / "results"),
                    "--llm-model",
                    "gpt-4o",
                    "--batch",
                ],
            )

        assert result.exit_code == 0
        mock_batch.assert_called_once()

    def test_run_without_batch_invokes_normal_pipeline(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title='Shoe',\n"
            "        description='A shoe',\n"
            "        category='Shoes', price=50.0, position=0)]\n"
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        mock_client = Mock(spec=LLMClient)
        mock_client.complete.return_value = LLMResponse(
            content="SCORE: 2\nREASONING: Good match",
            model="test-model",
            input_tokens=100,
            output_tokens=50,
        )

        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--config-name",
                    "test",
                    "--backend",
                    "file",
                    "--output-dir",
                    str(tmp_path / "results"),
                    "--llm-model",
                    "test-model",
                ],
            )

        assert result.exit_code == 0
        # Should have called complete() (sync path), not submit_batch()
        mock_client.complete.assert_called()

    def test_run_batch_dual_config(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_a = tmp_path / "adapter_a.py"
        adapter_a.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title='Shoe',\n"
            "        description='A shoe',\n"
            "        category='Shoes', price=50.0, position=0)]\n"
        )
        adapter_b = tmp_path / "adapter_b.py"
        adapter_b.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-2', title='Boot',\n"
            "        description='A boot',\n"
            "        category='Shoes', price=60.0, position=0)]\n"
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient

        mock_client = Mock(spec=LLMClient)
        mock_client.supports_batch.return_value = True

        mock_metrics = [
            MetricResult(metric_name="ndcg@10", value=0.8),
            MetricResult(metric_name="mrr", value=0.7),
        ]
        dual_return = ([], [], [], [], mock_metrics, mock_metrics, [], [], [])

        with (
            patch("veritail.cli.create_llm_client", return_value=mock_client),
            patch(
                "veritail.cli.run_dual_batch_evaluation",
                return_value=dual_return,
            ) as mock_dual_batch,
        ):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_a),
                    "--adapter",
                    str(adapter_b),
                    "--config-name",
                    "a",
                    "--config-name",
                    "b",
                    "--backend",
                    "file",
                    "--output-dir",
                    str(tmp_path / "results"),
                    "--llm-model",
                    "gpt-4o",
                    "--batch",
                ],
            )

        assert result.exit_code == 0
        mock_dual_batch.assert_called_once()

    def test_run_requires_queries_or_prefixes(self, tmp_path):
        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text("def search(q): return []\n")

        runner = CliRunner()
        result = runner.invoke(
            main,
            [
                "run",
                "--adapter",
                str(adapter_file),
                "--llm-model",
                "test-model",
            ],
        )
        assert result.exit_code != 0
        assert "Provide --queries, --prefixes, or both" in result.output

    def test_run_autocomplete_only(self, tmp_path):
        """--prefixes + --adapter with suggest() works without --llm-model."""
        prefixes_file = tmp_path / "prefixes.csv"
        prefixes_file.write_text("prefix,type\nrun,short_prefix\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import AutocompleteResponse\n"
            "def suggest(prefix):\n"
            "    return AutocompleteResponse(suggestions=['running shoes'])\n"
        )

        runner = CliRunner()
        result = runner.invoke(
            main,
            [
                "run",
                "--prefixes",
                str(prefixes_file),
                "--adapter",
                str(adapter_file),
                "--config-name",
                "ac-test",
                "--output-dir",
                str(tmp_path / "results"),
            ],
        )
        assert result.exit_code == 0
        assert "Autocomplete HTML report" in result.output
        assert (tmp_path / "results" / "ac-test" / "autocomplete-report.html").exists()

    def test_run_adapter_missing_suggest(self, tmp_path):
        """--prefixes provided but adapter lacks suggest() -> clear error."""
        prefixes_file = tmp_path / "prefixes.csv"
        prefixes_file.write_text("prefix,type\nrun,short_prefix\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text("def search(q): return []\n")

        runner = CliRunner()
        result = runner.invoke(
            main,
            [
                "run",
                "--prefixes",
                str(prefixes_file),
                "--adapter",
                str(adapter_file),
            ],
        )
        assert result.exit_code != 0
        assert "suggest()" in result.output

    def test_run_help_shows_prefixes_option(self):
        runner = CliRunner()
        result = runner.invoke(main, ["run", "--help"])
        assert result.exit_code == 0
        assert "--prefixes" in result.output
        assert "--suggest-checks" in result.output

    def test_run_both_search_and_autocomplete(self, tmp_path):
        """--queries + --prefixes + --adapter (with both functions) works."""
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        prefixes_file = tmp_path / "prefixes.csv"
        prefixes_file.write_text("prefix,type\nrun,short_prefix\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult, AutocompleteResponse\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title='Shoe',\n"
            "        description='A shoe',\n"
            "        category='Shoes', price=50.0, position=0)]\n"
            "def suggest(prefix):\n"
            "    return AutocompleteResponse(suggestions=['running shoes'])\n"
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        mock_client = Mock(spec=LLMClient)
        mock_client.complete.return_value = LLMResponse(
            content="SCORE: 2\nREASONING: Good match",
            model="test-model",
            input_tokens=100,
            output_tokens=50,
        )

        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--prefixes",
                    str(prefixes_file),
                    "--adapter",
                    str(adapter_file),
                    "--config-name",
                    "both-test",
                    "--backend",
                    "file",
                    "--output-dir",
                    str(tmp_path / "results"),
                    "--llm-model",
                    "test-model",
                ],
            )

        assert result.exit_code == 0
        exp_dir = tmp_path / "results" / "both-test"
        assert (exp_dir / "report.html").exists()
        assert (exp_dir / "autocomplete-report.html").exists()

    def test_init_autocomplete_creates_unified_adapter(self, tmp_path):
        """init --autocomplete generates adapter.py with both search() and suggest()."""
        runner = CliRunner()
        result = runner.invoke(main, ["init", "--dir", str(tmp_path), "--autocomplete"])

        assert result.exit_code == 0

        adapter_path = tmp_path / "adapter.py"
        prefixes_path = tmp_path / "prefixes.csv"

        assert adapter_path.exists()
        assert prefixes_path.exists()
        # No separate suggest_adapter.py
        assert not (tmp_path / "suggest_adapter.py").exists()

        adapter_text = adapter_path.read_text(encoding="utf-8")
        assert "def search(query: str) -> SearchResponse:" in adapter_text
        assert "def suggest(prefix: str) -> AutocompleteResponse:" in adapter_text
