"""Tests for the CLI interface."""

from __future__ import annotations

from click.testing import CliRunner

from veritail.cli import main


class TestCLI:
    def test_init_creates_starter_files(self, tmp_path):
        runner = CliRunner()
        result = runner.invoke(main, ["init", "--dir", str(tmp_path)])

        assert result.exit_code == 0
        assert "Created" in result.output

        adapter_path = tmp_path / "adapter.py"
        queries_path = tmp_path / "queries.csv"

        assert adapter_path.exists()
        assert queries_path.exists()

        adapter_text = adapter_path.read_text(encoding="utf-8")
        assert "def search(query: str) -> list[SearchResult]:" in adapter_text
        assert "urlopen(request, timeout=10)" in adapter_text
        assert "SEARCH_API_URL" in adapter_text

        queries_text = queries_path.read_text(encoding="utf-8")
        assert "broad" in queries_text
        assert "navigational" in queries_text
        assert "long_tail" in queries_text
        assert "attribute" in queries_text

    def test_init_refuses_to_overwrite_without_force(self, tmp_path):
        adapter_path = tmp_path / "adapter.py"
        adapter_path.write_text("old adapter\n", encoding="utf-8")

        runner = CliRunner()
        result = runner.invoke(main, ["init", "--dir", str(tmp_path)])

        assert result.exit_code != 0
        assert "Refusing to overwrite existing file(s)" in result.output
        assert "--force" in result.output
        assert adapter_path.read_text(encoding="utf-8") == "old adapter\n"

    def test_init_force_overwrites_existing_files(self, tmp_path):
        adapter_path = tmp_path / "adapter.py"
        queries_path = tmp_path / "queries.csv"
        adapter_path.write_text("old adapter\n", encoding="utf-8")
        queries_path.write_text("old queries\n", encoding="utf-8")

        runner = CliRunner()
        result = runner.invoke(main, ["init", "--dir", str(tmp_path), "--force"])

        assert result.exit_code == 0
        content = adapter_path.read_text(encoding="utf-8")
        assert "Generated by `veritail init`" in content
        assert "query,type,category" in queries_path.read_text(encoding="utf-8")

    def test_init_supports_custom_file_names(self, tmp_path):
        runner = CliRunner()
        result = runner.invoke(
            main,
            [
                "init",
                "--dir",
                str(tmp_path),
                "--adapter-name",
                "my_adapter.py",
                "--queries-name",
                "my_queries.csv",
            ],
        )

        assert result.exit_code == 0
        assert (tmp_path / "my_adapter.py").exists()
        assert (tmp_path / "my_queries.csv").exists()

    def test_init_validates_file_extensions(self, tmp_path):
        runner = CliRunner()
        result = runner.invoke(
            main,
            ["init", "--dir", str(tmp_path), "--adapter-name", "adapter.txt"],
        )

        assert result.exit_code != 0
        assert "--adapter-name must end with .py" in result.output

    def test_help(self):
        runner = CliRunner()
        result = runner.invoke(main, ["--help"])
        assert result.exit_code == 0
        assert "veritail" in result.output

    def test_run_help(self):
        runner = CliRunner()
        result = runner.invoke(main, ["run", "--help"])
        assert result.exit_code == 0
        assert "--queries" in result.output
        assert "--adapter" in result.output
        assert "--config-name" in result.output
        assert "--vertical" in result.output

    def test_run_mismatched_adapters_and_configs(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")
        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text("def search(q): return []\n")

        runner = CliRunner()
        result = runner.invoke(
            main,
            [
                "run",
                "--queries",
                str(queries_file),
                "--adapter",
                str(adapter_file),
                "--config-name",
                "a",
                "--config-name",
                "b",
            ],
        )
        assert result.exit_code != 0
        assert "matching --config-name" in result.output

    def test_run_dual_config_with_only_one_name_fails(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_a = tmp_path / "adapter_a.py"
        adapter_a.write_text("def search(q): return []\n")
        adapter_b = tmp_path / "adapter_b.py"
        adapter_b.write_text("def search(q): return []\n")

        runner = CliRunner()
        result = runner.invoke(
            main,
            [
                "run",
                "--queries",
                str(queries_file),
                "--adapter",
                str(adapter_a),
                "--adapter",
                str(adapter_b),
                "--config-name",
                "only-one-name",
            ],
        )
        assert result.exit_code != 0
        assert "matching --config-name" in result.output

    def test_run_rejects_top_k_less_than_one(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text("def search(q): return []\n")

        runner = CliRunner()
        result = runner.invoke(
            main,
            [
                "run",
                "--queries",
                str(queries_file),
                "--adapter",
                str(adapter_file),
                "--top-k",
                "0",
            ],
        )
        assert result.exit_code != 0
        assert "--top-k must be >= 1." in result.output

    def test_run_single_config_with_file_backend(self, tmp_path, monkeypatch):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title='Shoe',\n"
            "        description='A shoe',\n"
            "        category='Shoes', price=50.0, position=0)]\n"
        )

        # Mock the LLM client to avoid needing real API keys
        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        mock_client = Mock(spec=LLMClient)
        mock_client.complete.return_value = LLMResponse(
            content="SCORE: 2\nREASONING: Good match",
            model="test-model",
            input_tokens=100,
            output_tokens=50,
        )

        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--config-name",
                    "test",
                    "--backend",
                    "file",
                    "--output-dir",
                    str(tmp_path / "results"),
                    "--llm-model",
                    "test-model",
                ],
            )

        assert result.exit_code == 0
        assert "ndcg" in result.output.lower() or "Evaluating" in result.output

    def test_run_single_config_auto_generates_config_name(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "my_adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title='Shoe',\n"
            "        description='A shoe',\n"
            "        category='Shoes', price=50.0, position=0)]\n"
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        mock_client = Mock(spec=LLMClient)
        mock_client.complete.return_value = LLMResponse(
            content="SCORE: 2\nREASONING: Good match",
            model="test-model",
            input_tokens=100,
            output_tokens=50,
        )

        output_dir = tmp_path / "results"
        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--backend",
                    "file",
                    "--output-dir",
                    str(output_dir),
                    "--llm-model",
                    "test-model",
                ],
            )

        assert result.exit_code == 0
        assert "No --config-name provided. Using generated names:" in result.output

        experiment_dirs = [p for p in output_dir.iterdir() if p.is_dir()]
        assert len(experiment_dirs) == 1
        assert experiment_dirs[0].name.startswith("my-adapter-")
        assert (experiment_dirs[0] / "metrics.json").exists()
        assert (experiment_dirs[0] / "report.html").exists()

    def test_run_help_shows_checks_option(self):
        runner = CliRunner()
        result = runner.invoke(main, ["run", "--help"])
        assert result.exit_code == 0
        assert "--checks" in result.output

    def test_run_with_custom_checks(self, tmp_path):
        queries_file = tmp_path / "queries.csv"
        queries_file.write_text("query\nshoes\n")

        adapter_file = tmp_path / "adapter.py"
        adapter_file.write_text(
            "from veritail.types import SearchResult\n"
            "def search(q):\n"
            "    return [SearchResult(\n"
            "        product_id='SKU-1', title='Shoe',\n"
            "        description='A shoe',\n"
            "        category='Shoes', price=50.0, position=0)]\n"
        )

        check_file = tmp_path / "my_checks.py"
        check_file.write_text(
            "from veritail.types import CheckResult, QueryEntry, SearchResult\n"
            "\n"
            "def check_custom(\n"
            "    query: QueryEntry, results: list[SearchResult]\n"
            ") -> list[CheckResult]:\n"
            "    return [CheckResult(\n"
            "        check_name='custom',\n"
            "        query=query.query,\n"
            "        product_id=None,\n"
            "        passed=True,\n"
            "        detail='ok',\n"
            "    )]\n"
        )

        from unittest.mock import Mock, patch

        from veritail.llm.client import LLMClient, LLMResponse

        mock_client = Mock(spec=LLMClient)
        mock_client.complete.return_value = LLMResponse(
            content="SCORE: 2\nREASONING: Good match",
            model="test-model",
            input_tokens=100,
            output_tokens=50,
        )

        with patch("veritail.cli.create_llm_client", return_value=mock_client):
            runner = CliRunner()
            result = runner.invoke(
                main,
                [
                    "run",
                    "--queries",
                    str(queries_file),
                    "--adapter",
                    str(adapter_file),
                    "--config-name",
                    "test",
                    "--backend",
                    "file",
                    "--output-dir",
                    str(tmp_path / "results"),
                    "--llm-model",
                    "test-model",
                    "--checks",
                    str(check_file),
                ],
            )

        assert result.exit_code == 0
        assert "Loaded 1 custom check(s)" in result.output

    def test_version(self):
        runner = CliRunner()
        result = runner.invoke(main, ["--version"])
        assert result.exit_code == 0
        assert "0.1.0" in result.output
